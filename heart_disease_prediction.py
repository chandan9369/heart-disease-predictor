# -*- coding: utf-8 -*-
"""Heart_Disease_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KkKi4s1aXNrvA0LjvbcrX4ZkpUHbSNU4

# Heart Disease Prediction

# Overview

Machine Learning stands as a pivotal and extensively applied paradigm globally, with profound implications across various domains. Particularly, its significance in ***healthcare*** is undeniable, poised to revolutionize diagnostic processes for physicians.

This model delves into the application of Machine Learning within the healthcare domain, focusing on the analysis and prediction of **"heart disease using a dedicated dataset"**.

**Objective:**

The objective is to develop a robust model capable of discerning whether a patient exhibits signs of heart disease or falls within the normal range. By harnessing the power of Machine Learning, this predictive approach not only enhances the speed of diagnostics but also augments the overall efficiency of healthcare processes. The integration of advanced algorithms (like logistic regression and random forest classifier) and predictive analytics holds the promise of streamlining what would otherwise be a time-intensive diagnostic procedure, thereby contributing to more timely and accurate medical interventions.

## Key Takeaways

- This process involves data cleaning, data preprocessing, EDA(exploratory data analysis), and model evaluation steps.
- In my model, i have used 2 machine learning algorithms which will result in performance metrics of the model.
- The well-doing algorithm is implemented in the model and checking results with the real-time data.

# Table of Content

1. Importing Basic Libraries
2. Data Acquisition
3. EDA (Exploratory Data Analyis)
4. Visualization and Analysis
5. Splitting & Scaling Data
6. Choosing Model
   - Logistic Regression
   - Random Forest Classifier  
7. Training the Model
8. Comparision of Model

## 1. Importing Basic Libraries

Downloading all the required libraries which we are going to use in this ml model
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix

import os
import warnings
warnings.filterwarnings('ignore')

"""## 2. Data Aquisition

Reading the Data file
"""

heart_data = pd.read_csv('/content/heart.csv')

heart_data

"""## 3. EDA(Exploratory Data Analysis) üîé
Exploring the data set in order to derive useful information
"""

heart_data.columns

"""### Attributes:

- age: age in years
- sex: (1 = male; 0 = female)
- cp: chest pain type (4 values)
- testbps: resting blood pressure (in mm Hg on admission to the hospital)
- chol: serum cholestoral in mg/dl
- fbs: fasting blood sugar > 120 mg/dl (1 = true; 0 = false)
- restecg: resting electrocardiographic results (values 0,1,2)
- thalach: maximum heart rate achieved
- exang: exercise induced angina (1 = yes; 0 = no)
- oldpeak:  ST depression induced by exercise relative to rest
- slope: the slope of the peak exercise ST segment
- ca: number of major vessels (0-3) colored by flourosopy
- thal: 1 = normal; 2 = fixed defect; 3 = reversable defect
- traget: 1 (defective) and 0(Not Defective)
"""

heart_data.shape

heart_data.describe()

"""The `dataframe.describe()` function is used to obtain statistical information about a dataset. It provides details about the mean, standard deviation, maximum value, and more. This statistical information is presented in numerical format.

For instance, by analyzing the `AGE` column data, we can observe that the age of the youngest individual is 29 years, while the oldest is 77 years, with an average age of 54 years. The quartile details are given in 25%, 50%, and 75% values, indicating that the data is divided into three quartiles or four equal parts, with 25% of the values falling into each group.

While standard deviation and mean are statistical measures used to determine the central tendency of the dataset, it's important to note that the mean can be influenced by outliers. Hence, it is necessary to gather additional information before making accurate decisions.

### 3.1 Checking for Duplicates
"""

heart_data.duplicated().sum()

"""Here, we can see there is one duplicate present in whole dataset.

So, now we will drop this duplicate to clean our data.
"""

heart_data.drop_duplicates(inplace=True)

"""We have successfully deleted the duplicates present in the dataset. Now, we do not have any duplicates present in the dataset.

### 3.2 Checking for Misssing Values Present in the Dataset
"""

heart_data.isnull().sum()

"""So, we can notice here that there is no missing value present in the dataset."""

heart_data.nunique()

heart_data.info()

"""from above, we notice that the dataset has no null values which saved us from converting the null values into some data or dropping.

here is blog about how we can deal with null values in dataset.
- **Refer:** https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python

## Visualization and Analysis

### Correlation Among Attributes of Dataset
"""

plt.figure(figsize=(10,10))
sns.heatmap(heart_data.corr(), annot=True, cmap='terrain')

"""**Observation:**
- Correlation between target and cp,thalach, slope, and restecg are positive.
- Correlation between target and rest features are negative.

#### Countplot of Age vs Target
"""

plt.figure(figsize=(15,6))
sns.countplot(x="age", hue="target", data=heart_data, palette='Set1')

"""### Chest Pain Type(‚Äúcp‚Äù) Analysis"""

plt.figure(figsize=(8, 6))
ax = sns.countplot(x='cp', data=heart_data, hue='target', palette='Set1')
plt.title('Distribution of cp in Heart Data')
plt.xlabel('Chest Pain type')
plt.ylabel('Count')
plt.xticks(rotation=45)

"""Inference: As seen, there are 4 types of chest pain

1. status at least
2. condition slightly distressed
3. condition medium problem
4. condition too bad

#### Sex vs Chol Analysis
"""

sns.barplot(data=heart_data, x='sex', y='chol', hue='target', palette='spring')

"""#### Analyzing CP vs Target Feature"""

plt.figure(figsize=(15, 6))
ax = sns.countplot(x='cp', hue='target', data=heart_data, palette='spring')
plt.title('Distribution of CP vs Target in Heart Data')
plt.xlabel('CP')
plt.ylabel('Target')
plt.xticks(rotation=45)

"""The graph shows us that:
- People who don't have a lot of chest pain probably don't have heart disease.
- People who have really bad chest pain probably have heart disease.
- Old folks get chest pain more often.

#### Distribution of Target
"""

plt.figure(figsize=(10, 8))
ax = sns.countplot(x='target', data=heart_data,hue='target', palette='BuGn')
plt.title('Distribution of target in Heart Data')
plt.xlabel('target')
plt.ylabel('Count')
plt.xticks(rotation=45)

"""Here,

- **1 :** represent defective hearts
- **0 :** represent healthy heart

Now we will be using the standard scaler method to scale down the data so that it won‚Äôt raise the outliers also dataset which is scaled to general units leads to having better accuracy.

### Age vs Cholestrol Level
"""

plt.scatter(heart_data['age'], heart_data['chol'])
plt.xlabel("Age")
plt.ylabel("Target")

"""### Feature Engineering

#### Different Classes/Category of Features
"""

categorical_val = []

continuous_val = []

for column in heart_data.columns:
    print("--------------------")
    print(f"{column} : {heart_data[column].unique()}")

    if len(heart_data[column].unique()) <= 10:
      categorical_val.append(column)
    else:
      continuous_val.append(column)

categorical_val.remove('target')

df = pd.get_dummies(heart_data, columns=categorical_val)

"""Now we will be using the standard scaler method to scale down the data so that it won‚Äôt raise the outliers also dataset which is scaled to general units leads to having better accuracy.

## Preparing the Data for Model

### Scaling the Data
"""

sc = StandardScaler()

columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

heart_data[columns_to_scale] = sc.fit_transform(heart_data[columns_to_scale])

heart_data

# Now split the data into input data and output data

# input features
X = heart_data.drop(columns='target', axis=1)

# output features
Y = heart_data['target']

"""### Splitting the Data into Training data & Test Data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=40)

print("X_train: ", X_train.size)
print("Y_train: ", Y_train.size)
print("X_test: ", X_test.size)
print("Y_test: ", Y_test.size)

"""## Model Training"""

# function for calculating accuracy score for model based on
# their confusion matrix

def accuracy(cm):
  TP = cm[0][0]
  TN = cm[1][1]
  FN = cm[0][1]
  FP = cm[1][0]

  score = (TP + TN)/(TP + TN + FN + FP)

  return score

"""### 1. Logistic Regression"""

lr = LogisticRegression()

# training the LogisticRegression model with Training data
alg_lr=lr.fit(X_train, Y_train)

"""#### Predicting the Output for Test Data"""

Y_pred_lr = alg_lr.predict(X_test)

Y_pred_lr

"""####  Checking Accuracy Score through Confusion Metrics"""

cm_lr = confusion_matrix(Y_test, Y_pred_lr)
cm_lr

sns.heatmap(cm_lr, annot=True,cmap='BuPu')

"""Based on the analysis, we can conclude that our logistic regression model accurately predicted 39 cases of defective heart patients, while incorrectly predicting 7 cases. Although the number of incorrect predictions is not ideal, it is still acceptable.

Moreover, the model was able to correctly predict 49 cases of healthy patients, with only 6 incorrect predictions. Overall, the model seems to be performing well in predicting both defective and healthy heart patients.

#### Accuracy Score
"""

print('Accuracy Score of Logistic Model: ', accuracy(cm_lr))

"""### 2. Decision Tree Classifier


"""

from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier()

# training the LogisticRegression model with Training data
alg_dtree=dtree.fit(X_train, Y_train)

"""#### Prediction the Output"""

Y_pred_dtree = alg_dtree.predict(X_test)
Y_pred_dtree

"""#### Confusion Matrix for Model"""

cm_dtree = confusion_matrix(Y_test, Y_pred_dtree)
cm_dtree

sns.heatmap(cm_dtree, annot=True,cmap='BuPu')

"""#### Accuracy Score for Decision Tree Classifier Model"""

print('Accuracy Score of DecisionTree Classifier Model: ', accuracy(cm_dtree))

"""### 3. Random Forest Classifier"""

rf = RandomForestClassifier(n_estimators = 100)

alg_rf=rf.fit(X_train, Y_train)

"""#### Predicting Output"""

Y_pred_rf = alg_rf.predict(X_test)
Y_pred_rf

"""#### Confusion Matrix for Random Forest Model"""

cm_rf = confusion_matrix(Y_test, Y_pred_dtree)

cm_rf

sns.heatmap(cm_rf, annot=True,cmap='BuPu')

"""#### Accuracy Score for Random Forest Classifier Model"""

print('Accuracy Score of Random Forest Model: ', accuracy(cm_rf))

"""## Comparison Between Different Model

### Logistic Regression

- Accuracy Score of Logistic Model:  `0.8712871287128713`

### DecisionTree Classifier Mode

- Accuracy Score of DecisionTree Classifier Model:  `0.7821782178217822`

### Random Forest Classifier Model

- Accuracy Score of Random Forest Model:  `0.7821782178217822`

So, we can clearly see logistic regression classification is giving more accuracy than other model.

## Conclusion on Heart Disease Prediction

We have concluded our analysis on predicting `heart disease`. Our analysis included `data visualization, data analysis of the target variable, age features, univariate analysis` and `bivariate analysis`.

Additionally, we performed feature engineering to improve the accuracy of the model. Based on the results, we found that Logistic Regression achieved the highest accuracy of 87.12%.

## Building a Predictive System
"""

input_data = (57,0,1,130,236,0,0,174,0,0,1,1,2)

input_data_as_np_array= np.asarray(input_data)

input_data_reshaped = input_data_as_np_array.reshape(1,len(input_data_as_np_array))

prediction = alg_rf.predict(input_data_reshaped)

if (prediction[0]== 0):
  print('Person does not have a Heart Disease')
else:
  print('Person has Heart Disease')

"""## Saving the trained model"""

import pickle

# saving the trained model
filename = 'trained_model.pkl'
pickle.dump(alg_rf, open(filename, 'wb'))

loaded_model = pickle.load(open('trained_model.pkl','rb'))

input_data = (44,1,1,120,263,0,1,173,0,0,2,0,3)

input_data_as_np_array= np.asarray(input_data)
input_data_reshaped = input_data_as_np_array.reshape(1, len(input_data_as_np_array))


prediction = loaded_model.predict(input_data_reshaped)

if (prediction[0]== 0):
  print('Person does not have a Heart Disease')
else:
  print('Person has Heart Disease')